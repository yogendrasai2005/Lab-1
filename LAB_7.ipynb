{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a836a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Results:\n",
      "Best Perceptron Model: Perceptron(alpha=0.001, penalty='elasticnet')\n",
      "Best Perceptron Score: 0.2478723404255319\n",
      "Best MLP Model: MLPClassifier(alpha=0.001, max_iter=1000)\n",
      "Best MLP Score: 0.2968085106382979\n",
      "\n",
      "Detailed Performance Metrics for Best Perceptron Model on Test Set:\n",
      "{'Accuracy': 0.23728813559322035, 'F1-Score': 0.22142729172855014, 'Precision': 0.28467467408145375, 'Recall': 0.23728813559322035}\n",
      "\n",
      "Detailed Performance Metrics for Best MLP Model on Test Set:\n",
      "{'Accuracy': 0.25, 'F1-Score': 0.23920735006648416, 'Precision': 0.2760751427678279, 'Recall': 0.25}\n",
      "\n",
      "Classifier Evaluation Results:\n",
      "           Model  Accuracy    StdDev\n",
      "0     Perceptron  0.256361  0.017981\n",
      "1            MLP  0.294639  0.036626\n",
      "2            SVM  0.316989  0.021672\n",
      "3  Decision Tree  0.216988  0.024545\n",
      "4  Random Forest  0.290399  0.021905\n",
      "5       AdaBoost  0.216998  0.015509\n",
      "6    Naive Bayes  0.243578  0.037720\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.exceptions import ConvergenceWarning  # Import ConvergenceWarning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Check if optional libraries are available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    XGBClassifier = None\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError:\n",
    "    CatBoostClassifier = None\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_excel(file_path)\n",
    "    X = data.drop('Final_Marks', axis=1)\n",
    "    y = data['Final_Marks']\n",
    "    return X, y\n",
    "\n",
    "# Function to perform cross-validation and hyperparameter tuning using RandomizedSearchCV\n",
    "def tune_hyperparameters(X, y, model, param_distributions, n_iter=10):  # Reduce n_iter to speed up\n",
    "    search = RandomizedSearchCV(model, param_distributions, n_iter=n_iter, scoring='accuracy', cv=5, random_state=42)\n",
    "    search.fit(X, y)\n",
    "    return search.best_estimator_, search.best_score_\n",
    "\n",
    "# Function to evaluate and compare various classifiers\n",
    "def evaluate_classifiers(X, y):\n",
    "    classifiers = {\n",
    "        'Perceptron': Perceptron(),\n",
    "        'MLP': MLPClassifier(max_iter=1000),  # Increase max_iter to avoid convergence issues\n",
    "        'SVM': SVC(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "\n",
    "    # Add XGBoost if available\n",
    "    if XGBClassifier is not None:\n",
    "        classifiers['XGBoost'] = XGBClassifier()\n",
    "\n",
    "    # Add CatBoost if available\n",
    "    if CatBoostClassifier is not None:\n",
    "        classifiers['CatBoost'] = CatBoostClassifier()\n",
    "\n",
    "    results = []\n",
    "    for name, clf in classifiers.items():\n",
    "        scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')  # Reduce cv to 3 for faster evaluation\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': np.mean(scores),\n",
    "            'StdDev': np.std(scores)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Function to calculate detailed performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F1-Score': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'Precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "# Main program\n",
    "def main():\n",
    "    file_path = r\"C:\\Users\\SISTLA RAHUL\\Desktop\\cs_embed_data.xlsx\"\n",
    "    X, y = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Hyperparameter tuning for Perceptron model\n",
    "    perceptron_params = {'penalty': ['l2', 'elasticnet'], 'alpha': [1e-4, 1e-3, 1e-2, 1e-1]}\n",
    "    best_perceptron, best_score_perceptron = tune_hyperparameters(X_train, y_train, Perceptron(), perceptron_params, n_iter=8)\n",
    "    \n",
    "    # Hyperparameter tuning for MLP (Multi-Layer Perceptron) model\n",
    "    mlp_params = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['tanh', 'relu'], 'alpha': [1e-4, 1e-3]}\n",
    "    best_mlp, best_score_mlp = tune_hyperparameters(X_train, y_train, MLPClassifier(max_iter=1000), mlp_params, n_iter=8)\n",
    "    \n",
    "    # Evaluate various classifiers\n",
    "    evaluation_results = evaluate_classifiers(X_train, y_train)\n",
    "    \n",
    "    # Train and evaluate the best Perceptron model on the test set\n",
    "    best_perceptron.fit(X_train, y_train)\n",
    "    perceptron_predictions = best_perceptron.predict(X_test)\n",
    "    perceptron_metrics = calculate_metrics(y_test, perceptron_predictions)\n",
    "    \n",
    "    # Train and evaluate the best MLP model on the test set\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "    mlp_predictions = best_mlp.predict(X_test)\n",
    "    mlp_metrics = calculate_metrics(y_test, mlp_predictions)\n",
    "    \n",
    "    # Display the results of hyperparameter tuning and model evaluation\n",
    "    print(\"Hyperparameter Tuning Results:\")\n",
    "    print(\"Best Perceptron Model:\", best_perceptron)\n",
    "    print(\"Best Perceptron Score:\", best_score_perceptron)\n",
    "    print(\"Best MLP Model:\", best_mlp)\n",
    "    print(\"Best MLP Score:\", best_score_mlp)\n",
    "    \n",
    "    print(\"\\nDetailed Performance Metrics for Best Perceptron Model on Test Set:\")\n",
    "    print(perceptron_metrics)\n",
    "    \n",
    "    print(\"\\nDetailed Performance Metrics for Best MLP Model on Test Set:\")\n",
    "    print(mlp_metrics)\n",
    "    \n",
    "    print(\"\\nClassifier Evaluation Results:\")\n",
    "    print(evaluation_results)\n",
    "\n",
    "# Run the main program\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eeeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
